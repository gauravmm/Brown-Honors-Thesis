\documentclass[letterpaper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{qtree}
\usepackage{graphicx}
\usepackage{xfrac}
\usepackage{forest}
\usepackage{tikz-dependency}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{xfrac}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{pbox}
\usepackage{framed}
\usepackage{titlesec}
\usepackage[authordate,bibencoding=auto,strict,backend=biber,natbib]{biblatex-chicago}
\usepackage{enumitem}
\usepackage{todonotes}
\renewcommand{\cite}{\citep}

%\newcommand{\sectionbreak}{\clearpage}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% http://tex.stackexchange.com/a/33547
\newcommand{\approptoinn}[2]{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    #1\propto\cr\noalign{\kern2pt}#1\sim\cr\noalign{\kern-2pt}}}}}

\newcommand{\appropto}{\mathpalette\approptoinn\relax}

\addbibresource{prop.bib}

%opening
\title{Incrementally Identifying Objects from Referring Expressions using Spatial Object Models}
\author{Gaurav Manek}

\begin{document}

\maketitle


\section{Introduction}

In any task with robot-computer collaboration, the success of the task depends on the ability of robots to interact well with humans. Of particular interest is the problem of \emph{social feedback}, where the robot shows its understanding of a human's utterances by generating small responses as it listens to the human. Providing social feedback during human-robot interaction requires robots to be able to parse the human input \emph{incrementally}: updating its understanding as each next word is uttered. In this paper, we present an incremental model for parsing referring expressions.

Referring expressions are phrases used to identify a particular object in a scene by describing it and its relative position to other objects in the same scene.

Incremental parsing allows the robot to provide social feedback to the human. The robot could, for example, transition from a confused to a smiling face as more input words reduce the uncertainty of the referring expression. Referring expression parsing is also closely related to the common robot task of pick-and-place, and could thus be a useful improvement for the Baxter robot and the H2R lab. 

In current work, referring expression parsing is done in batch-mode, with the entire referring expression as input. The $G^3$ framework by \citet{tellex2011understanding}, work done by \citet{UW_RSE_ICML2012}, and the parser developed by \citet{artzi2013weakly}, all require a complete sentence. Since the robot is presented with the input one word at a time, batch-mode requires waiting for the complete utterance before processing and providing output, which takes time to complete. For example, practical implementations of the $G^3$ system can take up to 30 seconds from the end of the input to the start of a response. 

Our incremental parsing system updates the distribution with each added word, substantially reducing the delay between input and response.

We assume that we have access to a model that maps an arbitrary noun phrase (without prepositions) to a probability distribution over objects that the noun phrase may refer to.

(This thesis project is the culmination of work done from Fall 2014 to present. Some work was previously completed in collaboration with undergraduate student Zachary Loery.)


\subsection{Examples}
Parsing a referring expression and finding a distribution over possible objects has been accomplished by \citet{tellex2011understanding},\citet{UW_RSE_ICML2012}, and \citet{artzi2013weakly}, among others. Here we present some examples of referring expressions.

Given a \textit{red apple}, \textit{blue tape}, \textit{red bowl}, and a \textit{metal bowl} scattered on a table, we want to be able to convert referring expressions similar to these into probability distributions over the objects:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
 \item ``The object between the apple and the tape.''
 \item ``The red object.''
 \item ``The red thing on the left of the bowl.''
 \item ``The red thing on the left.''
 \item ``The thing on the left of the bowl.''
\end{enumerate}

We want to be able to parse these referring expressions incrementally. Given the referring expression ``The orange cube between the red and the yellow'' to refer to the indicated object in 
Figure~\ref{fig:worked_example_table}, our model should give better estimations of the correct object as we receive more words in the referring expression. Figure~\ref{fig:worked_example_incremental} shows how the distribution over all objects changes with more words of the referring expression.

\begin{figure}[h!tb]
  \centering
    \includegraphics[width=0.66\textwidth]{worked_example_scene}
  \caption{Arrangement of objects on a table.}
  \label{fig:worked_example_table}
\end{figure}
\begin{figure}[h!tb]
  \centering

\newcommand{\dist}[8]{\begin{tikzpicture}[trim axis left]
\begin{axis}[
	width=1.2in,
	height=0.6in,
	scale only axis,
	xticklabel=\empty,
	xtick=\empty,
	ytick=\empty,
	xlabel=Object,
	x label style={yshift=10pt},
	y label style={yshift=-24pt},
	ybar interval=0.9,
	enlarge x limits=-0.3,
	ymax=1.1, ymin=0.0,
	xmin=1, xmax=9,
]
\addplot 
	coordinates {(1,#1) (2,#2)
		 (3,#3) (4,#4) (5,#5) (6,#6) (7,#7) (8,#8) (9,0) };
\end{axis}
\end{tikzpicture}}

\begin{tabular}{cccc}
\dist{0.1}{0.1}{0.1}{0.1}{0.1}{0.1}{0.1}{0.1} & \dist{0.2}{0.01}{0.2}{0.2}{0.01}{0.01}{0.2}{0.01} &\dist{0.3}{0.01}{0.3}{0.1}{0.01}{0.01}{0.3}{0.01} & \dist{0.05}{0.}{0.8}{0.05}{0.}{0.}{0.05}{0.} \\
\textit{``The\ldots} &
\textit{\ldots orange cube\ldots} &
\textit{\ldots between the red\ldots} &
\textit{\ldots and the yellow.''} \\
\end{tabular}
  \caption{Sample probability distribution over all objects, updated incrementally.}
  \label{fig:worked_example_incremental}
\end{figure}


\section{Technical Approach}

Here we provide a top-down view of the algorithm, factoring the probability and explicitly noting the assumptions made at each step.

Given a sequence of words $w_1, w_2, \ldots, w_t$, we estimate $T$, the distribution of the object the user is referring to.
\begin{align*}
	& \Pr(T = o | w_1, w_2, \ldots, w_t)
\intertext{We then assume that we can factor the sequence of words into separate independent ranges, each of which corresponds to either a description of the object (e.g. ``the orange cube'') or a prepositional phrase (e.g. ``between the \ldots''). We assume these to be independent and factor the expression:}
   = & \Pr(X_1 = o \land X_2 = o \land \ldots | w_1, w_2, \ldots, w_t)
\\ = & \Pr(X_1 = o | w_1, w_2) \cdot \Pr(X_2 = o | w_3, w_4, w_5) \cdot \ldots
\end{align*}

The splitting of the sentence into separate groups is done using a chunking algorithm, which we discuss in Section \ref{sec:tagging}. We assume that the chunkings given here are certain, and so eliminate the probability term associated with that.

\begin{align*}
\intertext{Each of these expressions can either be describing the target object or describing the location of the target object using a preposition. In the former case, we use a unigram model (detailed in Section \ref{sec:unigram}) to estimate this probability.}
	\Pr(X_i = o | w_1, \ldots, w_k) & \appropto \prod_{w_i \in (w_0, w_1, \cdots)} \left( (1 - \alpha) * \frac{Q(o, w_i)}{\sum_{\Omega} Q(\Omega, w_i)} + \alpha * \frac{1}{k} \right)
\intertext{In the latter case, we convert the expression to a combination of a preposition and at least one other distribution over objects. We parameterize the distribution using a feature-vector generator $f$, weights $\theta_p$, normalizing factor $z$, and sigmoid functon $S$. The probabilities $\Pr(X_{i_j} = o_j | w_1, \ldots, w_k)$ are factored as above. Details of this process are in Section \ref{sec:bottomupeval}}.
\Pr(X_i = o | w_1, \ldots, w_k) & = \Pr(X_i = o | P = p, X_{i_1}, X_{i_2}, \ldots) \cdot \Pr(P = p, X_{i_1}, X_{i_2}, \ldots | w_1, \ldots, w_k)
\\ & \appropto \sum_{o_1, o_2, \cdots o_n \in O} \text{S}\left( \frac{f(o_1, o_2, \cdots, o_n) \cdot \theta_p}{z} \right) \cdot \prod_{j = 1}^n \Pr(X_{i_j} = o_j | w_1, \ldots, w_k)
\end{align*}


\subsection{Algorithm}

The algorithm we present uses the factorization presented above but operates in a bottom-up manner. We represent the referring expression as a tree, updating it each time we receive the next word from the user. This representation allows us to perform computationally-intensive tasks only once and cache intermediate results, allowing us to produce intermediate results without having to recompute them.

More precisely, we construct a semantic tree such that each leaf node in the tree is a noun-phrase that refers to some object. This tree is constructed by chunking the input using conditional random fields and then using deterministic transformations to turn the chunked input into a tree. We convert each of the leaf nodes into distributions over objects using a language model, and then finally evaluate this structure to obtain the final distribution. Here is a concrete example: 

We attempt to locate the \textit{orange cube} using the following referring expression: ``The orange cube between the red thing and the yellow thing.'' A selection of \textit{red}, \textit{blue}, \textit{orange}, \textit{purple}, \textit{green}, and  \textit{yellow} cubes are arranged on a table, as shown in Figure~\ref{fig:worked_example_table}. The orange cube referred to in the above statement is pointed to by a red arrow in the figure. The process of estimating the object from this information is:

\renewcommand{\tabularxcolumn}[1]{>{\small}m{#1}}
\begin{tabularx}{\textwidth}{@{} >{\hsize=.5\hsize \centering}X >{\hsize=.45\hsize}X @{}} 
\multicolumn{2}{c}{``The orange cube between the red thing and the yellow thing.''} \\ 
\multicolumn{2}{c}{$\Downarrow$} \\ 
% Chunking	
$\underbrace{\text{The orange cube}}_{\texttt{SNP}}$
$\underbrace{\text{between}}_{\texttt{PRP}}$
$\underbrace{\text{the red thing}}_{\texttt{SNP}}$
$\underbrace{\text{and}}_{\texttt{PRP}}$
$\underbrace{\text{the yellow thing}}_{\texttt{SNP}}$
$\underbrace{\text{.}}_{\texttt{.}}$
& 
\textbf{Chunking}

First, we use Mallet, \citep{McCallumMALLET} a sequence tagging library, to tag the referring expression with a set of tags. The tags were developed to suit our use case, and are a simplification of the typical English parts-of-speech tags.

\\ $\Downarrow$ \\[0.2cm] 

\scalebox{.9}{\Tree [.TARGET \qroof{The orange cube}.OBJECT [.BETWEEN   \qroof{the red thing}.OBJECT \qroof{the yellow thing}.OBJECT ] ]} &

\textbf{Semantic Tree}

The tagged sequence is then converted to a semantic tree. Because of the simplified tag set in the previous step, this transformation is deterministic.

\\ $\Downarrow$ \\[0.2cm] 

\scalebox{.9}{\Tree [.TARGET \qroof{$\{\sfrac{1}{16}, \sfrac{1}{16}, \ldots, \sfrac{1}{16}\}$}.OBJECT [.BETWEEN   \qroof{$\{0, \sfrac{1}{8}, \ldots, 0\}$}.OBJECT \qroof{$\{0, 0, \ldots, \sfrac{1}{2}\}$}.OBJECT ] ]} 

~

\scriptsize{\{\textit{orange\_1}, \textit{red\_1}, $\ldots $, \textit{yellow\_2}\}}

&
\textbf{Language Model}

We use a language model to convert each of the simple noun phrases into individual distributions over possible objects.

\\ $\Downarrow$ & ~ \\[0.2cm] 

\scalebox{.9}{\Tree [.TARGET \qroof{$\{90\%, 2\%, \ldots 1\%\}$}.OBJECT ]} 

~

\scriptsize{\{\textit{orange\_1}, \textit{orange\_2}, $\ldots $, \textit{blue\_1}\} }

&
\textbf{Bottom-up Evaluation}

We evaluate the tree from the bottom upwards, using spatial features and trained weights to convert each input distribution and the type of preposition into a set of output scores. The final result is the object with the highest score.
\end{tabularx}

% \subsection{Mathematical Models}
% We have already devised mathematical models to evaluate referring expressions in batch mode. One of the tasks in this project is to factor these models for incremental use.

\subsection{Chunking and Semantic Tree construction}
\label{sec:tagging}

We model the relationship between words and tags as a conditional random field, where the tag for any particular word depends on the neighboring words. We directly estimate the distribution of tags using the existing library Mallet, developed by \citet{McCallumMALLET}.

The transformation from the tagged sequence to the semantic tree is entirely deterministic, as the tags are tailored to the specific form of the queries in the corpus.

\subsection{Language Model for Objects}
\label{sec:langmod_obj}
\label{sec:unigram}

The language model used is a unigram language model, where each word $w_j$ refers to object $o$ with some joint score $Q(o, w_i)$ that we can estimate statistically. 

The distribution of simple noun phrase $w_1, \ldots, w_k$ referring to object $o$ is, therefore:

\begin{align*}
	\Pr(X_i = o | w_1, \ldots, w_k) & = \frac{\Pr(X_i = o \cap w_1, \ldots, w_k)}{\Pr(w_1, \ldots, w_k)}
\\ & \propto \Pr(X_i = o \cap w_1, \ldots, w_k)
\\ & \approx \prod_{w_i \in (w_0, w_1, \cdots)} \left( (1 - \alpha) * \frac{Q(o, w_i)}{\sum_{\Omega} Q(\Omega, w_i)} + \alpha * \frac{1}{k} \right)
\end{align*}

For smoothing, we assume that there is some small probability $\alpha$ that the object is uniformly at random chosen without regard to the word. We arbitrarily set $\alpha \approx 5\%$.

\subsection{Bottom-up Evaluation}
\label{sec:bottomupeval}
Once we have a semantic tree, we can simplify the tree in a bottom-up manner to obtain the final distribution. Figure~\ref{fig:bottom_up_eval} illustrates this process and the three possible cases we apply to convert the tree into a distribution over objects.

\begin{figure}[h!tb]
  \centering
\begin{tabular}{ccccccc}
\Tree [.$\circ$ \emph{The orange cube} [.{\emph{between}} \emph{the red} \emph{the yellow} ]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.$\circ$ $X_1$ [.{Preposition $p \in P$} $X_2$ $X_3$ ]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.{$\circ$} $X_1$ $T_1$ ]
&
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} & 
\Tree [.{$\circ$} $T_0$ ]
\\ 
\multicolumn{2}{r}{\textbf{(Case 0)}} & \multicolumn{2}{r}{\textbf{(Case 1)}} & \multicolumn{2}{r}{\textbf{(Case 2)}}
\end{tabular}
\caption{The three cases of bottom-up evaluation.}
  \label{fig:bottom_up_eval}
\end{figure}

\paragraph{Case 0} Simplifying noun-phrases into a distribution.

Each grounding in the tree is modeled by a distribution that is obtained from the language model. We have described how this is done in Section~\ref{sec:langmod_obj}. A simple implementation will be able to perform a lookup in $\mathcal{O}(|\bm w|)$ time, where $|\bm w|$ is the number of words in a single grounding.

\paragraph{Case 1} Simplifying a preposition and associated noun-phrases.

We have preposition $p \in P = $ \{near, left, right, front, behind, between\}. Each grounding that the preposition relies on is modeled by the distributions $X_1, X_2, \cdots X_n$, obtained from the language model. $T$ is the distribution of the object that the user is referring to.

In Figure~\ref{fig:bottom_up_eval}, we illustrate how we simplify preposition $p$ with groundings $X_1$ and $X_2$ into a single distribution $T_1$.

We find $T$ with:
\begin{align*}
\Pr(T = o) & = \Pr(T = o | X_1, X_2, \cdots X_n, P = p)
\end{align*}
\begin{align*}
\intertext{Assume that each grounding is independent and factor the expression:}
& \propto \sum_{o_1, o_2, \cdots o_n \in O} \Pr(T = o \cap P = p \cap X_1 = o_1, X_2 = o_2, \cdots X_n = o_n)
\\ & = \sum_{o_1, o_2, \cdots o_n \in O} \Pr(T = o \cap P = p | X_1 = o_1, X_2 = o_2, \cdots X_n = o_n) \cdot \Pr(X_1 = o_1, X_2 = o_2, \cdots X_n = o_n)
\intertext{Parametrize the probability with feature-vector function $f$, weights $\theta_p$, logistic function $S$, and some normalization factor $z$:}
& = \sum_{o_1, o_2, \cdots o_n \in O} \text{S}\left( \frac{f(o_1, o_2, \cdots, o_n) \cdot \theta_p}{z} \right) \cdot \prod_{i = 1}^n \Pr(X_i = o_i)
\end{align*}

We can estimate $\theta_p$ for each $p$ using logistic regression, and select feature-vector function $f$ separately.

We observe that the na\"ive implementation takes time to the order of $\mathcal{O}(|O|^{n+1})$, where $|O|$ is the number of objects and $n$ is the number of groundings that each preposition has.

\paragraph{Case 2} Combining multiple distributions.

In Figure~\ref{fig:bottom_up_eval}, we illustrate how we simplify groundings $X_1$ and independently derived distribution $T_1$ to get $T_0$, our estimated distribution.

In cases where we have multiple distributions, we estimate the true distribution by assuming each of $X_1, X_2, \cdots X_n$ are independent and taking the joint probability, assuming a uniform prior over all objects:
\begin{align*}
\Pr(T = o) & = \Pr(T = o | X_1, X_2, \cdots X_n)
\\ & = \Pr(X_1 = o, X_2 = o, \cdots X_n = o)
\\ & = \prod_{i = 1}^n\Pr(X_i = o)
\end{align*}

The na\"ive implementation also takes time to the order of $\mathcal{O}(|O|^{n+1})$, where $|O|$ is the number of objects and $n$ is the number of groundings for which the marginal distribution must be taken.

These cases are sufficient to recursively reduce the tree into a single distribution.

\subsection{Incremental Parsing}

In the previous section we factored the simplification of the semantic tree into three separate cases. We cache the result of each simplifying step, as illustrated in Figure~\ref{fig:incremental_caching}.

\begin{figure}[h!tb]
  \centering
\begin{tabular}{ccccc}
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache miss} \emph{The orange cube} ] ]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache hit} \emph{The orange cube} ] [.\fbox{cache miss} [.{\emph{between}} [.\fbox{cache miss} \emph{the red} ]]]]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache hit} \emph{The orange cube} ] [.\fbox{cache miss} [.{\emph{between}} [.\fbox{cache hit} \emph{the red} ] [.\fbox{cache miss} \emph{the yellow} ]]]]]
\\ \hline
``The orange cube & & between the red  & & and the yellow.''
\end{tabular}
\caption{Cache behavior as words are added.}
  \label{fig:incremental_caching}
\end{figure}

\subsubsection{Runtime Analysis}

We use the runtime analysis of each separate case to draw conclusions about the worst-case time taken for the algorithm to produce an updated distribution given one additional word.

Given the addition of one word, we need to make at most one more recursive simplification than the depth of the tree. In all our training data, the maximum tree depth observed is never more than two, so an upper bound of three simplifications per word input means that this algorithm can easily meet the runtime requirements of online algorithms.

More formally, given the runtimes and caching behavior discussed earlier, the worst-case time to update the distribution $T_0$ to include the next word from the user is $\mathcal{O}(|\bm w|) \prod_i^{k-1} \mathcal{O}(|O|^{n+1}) = \mathcal{O}(|\bm w| * |O|^{kn})$, where $k$ is the number of layers in the tree.

When $k = 3$ and $n \leq 2$, as in real-world examples, this instead evaluates to the very manageable $\mathcal{O}(|\bm w| * |O|^{4})$, where $|\bm w|$ is the number of words in the simple noun phrase, and $|O|$ is the number of objects in the scene.

\subsubsection{Chunking with Conditional Random Fields}

The use of conditional random fields complicates the analysis somewhat because adding a word can affect the tags of words already processed, which causes the tree structure to change. In this case, we recompute all nodes in the tree corresponding to words with changed tags, paying the recomputation penalty. Other models such as Hidden Markov Models also have this property.

We choose to use Conditional Random Fields in part because we can explicitly set the look-ahead limit, and so pick a tradeoff between tagging accuracy and potential worst-case performance. For our algorithm, we arbitrarily set it to 5.

The worst-case runtime scales linearly with the look-ahead limit. To characterize the typical performance penalty caused by our choice of look-ahead parameter, we gathered statistics on the likelihood of this happening in our training set.

We found that...

\fbox{\parbox[b][2em][t]{\textwidth}{\textbf{Todo} Characterize the penalty associated with CRF-retagging leading to recomputation. Some basic probing shows that it isn't much after the first three or four words, but I would like to characterize this better.}}

\subsection{Technical Approach Conclusion}

In this section, we presented a top-down factorization of the problem and mathematical steps for bottom-up evaluation of arbitrary referring expressions. We have also shown that our strategy gives good time guarantees in the worst case scenario, and so is suitable for realtime use.

Now we present the mechanism by which we train various model parameters.

\section{Learning Model Parameters} 

Training each model that we use requires a lot of data. We collected many examples to train and test our system on. We also had humans evaluate our test set to benchmark the performance of our system. We trained the language model and spatial-prepositional model on a training set of 11 scenes, each with an average of 14 objects, for a total of 417 input sentences. We trained the chunker on hand-annotated parses of these input sentences.

We have collected a corpus of human-generated data using Human Intelligence Tasks (HITs) on the Amazon Mechanical Turk (AMT) platform and hand-generated scenes. We additionally use AMT to have humans evaluate our test set to obtain a baseline.

 \subsection{Referring Expression Generation}
A set of HITs were created to elicit referring expressions. A total of 19 scenes were constructed, each with a set of about 12 to 15 objects scattered on a table and at least 6 identical orange cubes. For each orange cube in each scene, 9 different workers were told to ask a robot across the table for the indicated orange cube. Figure~\ref{fig:ref_expr_examples_pic} provides an example of such a labeled scene.

\begin{figure}[htb]
  \centering
    \includegraphics[width=0.75\textwidth]{ref_expr_examples_pic}
  \caption{Example picture provided to AMT workers to elicit referring expressions.}
  \label{fig:ref_expr_examples_pic}
\end{figure}

We provide the following instructions to respondents:

\begin{framed}
You are standing across the table from a robot. Write down what you would say to the robot if you wanted the indicated orange cube on the table.
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item Use phrases like "between", "near", "left of", "right of", "in front of" and "behind".
\item Use front/behind/left/right from the robot's perspective, as labeled in the image. 
\item All orange cubes look the same.
\item You must ask for the item indicated by the red arrow
\item The instruction must be a single sentence.
\end{itemize}
\end{framed}

\subsection{Referring Expression Evaluation}

For each referring expression in the test set, we get three separate human raters to identify the target and provide feedback on the ease of understanding of the referring expression. Figure~\ref{fig:ref_expr_eval_pic} shows an example scene.

\begin{figure}[h!tb]
  \centering
    \fbox{\includegraphics[width=0.90\textwidth]{ref_expr_eval_pic}}
  \caption{Example picture provided to AMT workers to evaluate referring expressions.}
  \label{fig:ref_expr_eval_pic}
\end{figure}

\subsection{Data Gathered}

We received referring expressions from human AMT workers. These referring expressions are some representative samples of responses to the scene in Figure~\ref{fig:ref_expr_examples_pic}:

\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item ``I want the orange cube in front of you, between the chili and the toy.''
	\item ``hand me the orange cube that is in between jalape\~{n}o and airplane''
	\item ``Directly in front of you next to the green pepper and airplane.''
	\item ``Take the orange block between the toy airplane and green chili pepper''
	\item ``bring the orange cube between the pepper and the plane.''
\end{itemize}

\section{Current Results}

After training our model on the training set, we tested it using a test set of 10 scenes, each with an average of 14 objects, for a total of 381 input sentences. The result of running the parser on complete referring expressions is in Figure~\ref{fig:results}.

\begin{figure}[H]
  \centering
  \begin{tabular}{| r r | c | c | c || c c |} \hline
     & & \multicolumn{5}{c|}{Rate of correct identification, Test (\%)} \\
     & & \multicolumn{3}{c||}{Baselines} & \multicolumn{2}{c|}{Results} \\
     \multicolumn{2}{|c|}{Preposition}
			                      &   Human & Unigram & Random &  Top-1 & Top-3 \\\hline
26.8\% & \textit{between}       & 88.2  & 14.3  & 7.1   & 71.6  & 94.1 \\
21.3\% & \textit{near}          & 82.5  & 17.2  & 7.3   & 7.9   & 51.1 \\
14.2\% & \textit{behind}        & 76.9  & 15.7  & 6.9   & 25.9  & 75.9 \\
11.0\% & \textit{in front of}   & 69.9  & 17.9  & 7.5   & 21.4  & 64.3 \\
9.4\% & \textit{left of}        & 89.8  & 16.1  & 7.3   & 25.0  & 83.3 \\
5.8\% & \textit{right of}       & 58.0  & 15.4  & 7.3   & 32.5  & 55.2 \\\hline\hline
    \multicolumn{2}{|r|}{Total} & 79.0  & 16.1  & 7.2   & 32.8  & 63.5 \\\hline
  \end{tabular}
  \caption{Performance on test set.}
  \label{fig:results}
\end{figure}

Figure~\ref{fig:results} shows the correctness rate of our algorithm and of three baselines for comparison. The percentage next to each preposition is the fraction of sentences in the test set that contain this preposition, and so will not add up to 100\%. The baselines are:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item The \emph{Human} baseline, which was established by having humans select the object best identified by the referring expression, and scoring them against our corpus.
	\item The \emph{Unigram} baseline, which is the expectation of selecting the correct object using a simple unigram object model across the entire input sentence.
	\item The \emph{Random} baseline, which is the expectation of selecting the correct object by selecting one object uniformly at random.
\end{enumerate}

The \emph{Results} column lists the rate of correct identification using the entire sentence as input, the \emph{Top-1} column lists the rate at which the correct object is rated the most likely by the algorithm, and the \emph{Top-3} column lists the rate at which the correct object is in the top 3 items.

For evaluation, the rate of correct identification is the number of trials in which the algorithm assigns a higher probability to the correct item than to any other item. Should there be a tie, the rate is divided by the number of items of equal probability.

The percentage on the left of each preposition is the fraction of the test set that contains that preposition.

\subsection{Incremental Performance}

To evaluate incremental performance, we report performance on the test set as a fraction of each sentence provided to the algorithm. Figure~\ref{fig:results_inc} reports the evaluation of the test set when our algorithm is run on it word-by-word. Note that, due to the variation in lengths of sentences, the charts are drawn by interpolating fraction of each sentence into bins from 0 to 1.

\begin{figure}[H]
  \centering
  \includegraphics[scale=.60]{eval/cdf_by_fraction}
  \caption{Performance on test set.}
  \label{fig:results_inc}
\end{figure}

Five separate lines are drawn to show the distribution of the rank of the correct option. Each Top-$k$ line includes an example if the target object has at least as much probability as the $k^\text{th}$-highest probability in the distribution. Should there be a tie, the rate is divided by the number of items of equal probability.

\subsection{Failure Analysis}
After testing was complete, we examined individual cases in the test set to characterize failures. 

\subsubsection{Spatial Model of `near'}

We observe that the rate of correct identification of our model is particularly low in comparison to that of humans when dealing with the preposition `near'. For each phrase ``[Figure] near [Landmark]'' in the training and test sets, we plot the location of the ``Landmark'' object in comparison to the target ``Figure'' object in in the training and test sets in Figure~\ref{fig:near_comp}. It is clear that this simple model does not fully capture the nuances of the use of `near'.

\begin{figure}[h!tb]
  \centering
    \includegraphics[width=0.75\textwidth]{near_train_test_comp}
  \caption{Relative position of ``[Figure] near [Landmark]'' in the training and the test sets, with the probability density of the model as background.}
  \label{fig:near_comp}
\end{figure}

Perhaps this model could be extended to include the relative positions of other objects, or to reflect the local geometry of the scene. We suspect that a major source of error is the difference in perspective of the robot and the human. The human perspective is in front of the table and so is subject to foreshortening, while the robot's perspective does not.

\subsubsection{Dealing with Imprecise and Incorrect Data}

Another major source of error is the presence of imprecise or incorrect data in our training and test set. An imprecise referring expression is one that does not uniquely identify a target object, and an incorrect referring expression is one that does not identify the correct object. Here we estimate how much of our error is due to imprecise and incorrect referring expressions by evaluating our algorithm on referring expressions with $r$ humans correctly identifying the target object.

Figure~\ref{fig:interrater} shows the outcome of this when we restrict the test set to referring expressions that have been correctly identified by an increasing number of humans. The first column is the control (using all data), the second includes only referring expressions correctly solved by at least one human, then at least two humans, and finally only those by all three humans.

\begin{figure}[h!tb]
  \centering
  \begin{tabular}{|r|m{1.2cm}m{1.2cm}m{1.2cm}m{1.2cm}|}\hline
  & \multicolumn{4}{c|}{Rate of correct identification, Test (\%)} \\
  No. of Correct Raters, $r$ & $\geq 0$ & $\geq 1$ & $\geq 2$ & $=3$ \\\hline
  Top-1 & 32.8 & 33.7 & 34.2 & 36.9 \\
  Top-3 & 63.5 & 65.3 & 65.9 & 69.6 \\\hline
  Fraction of total (\%) & 100 & 93.1 & 84.5 & 59.3 \\\hline
  \end{tabular}
  \caption{Correctness rate for different number of correct raters, with size of data.}
  \label{fig:interrater}
\end{figure}

From this we see that genuine confusion accounts for between 4-6 percentage points of the total error, which is substantial.

\section{Related Work}

Prepositional phrases have not been subject to as much computational analysis and study as noun- and verb-phrases. However, there are still a number of papers related to the topic. 

There is an existing family of related work by \citet{tellex2011understanding}, \citet{UW_RSE_ICML2012}, and \citet{artzi2013weakly}, all of whom present modern models to process referring expressions. These models all operate on entire input sentences and are designed to parse general instructions and commands instead of only prepositional phrases. 
\citet{tellex2011understanding} present the $G^3$ framework. We use several key ideas from this paper: in particular we implicitly assume the binary correspondence variable that their model maximizes. Our algorithm is inspired in part by the algorithm they present.
\citet{UW_RSE_ICML2012} present a state-of-the-art process to learn models for a semantic parser and word-classifier alignment. Our approach is substantially different from Matuszek~et.~al. since we do not separate perceptual features from the language model of each object. Also, in the learning phase of their algorithm, Matuszek~et.~al. calculate the marginal probability of a particular grounding and a particular word by performing a beam search over all possible parses. We assume instead that each node in the parse is independent of its sibling nodes, which allows us to use dynamic programming to incrementally build the distribution.
\citet{artzi2013weakly} train Combinatory Categorial Grammars (CCG) with ambiguous validation functions to parse instructions, including spatial relations. While this approach is more flexible and likely performs better on entirely novel sentences, we deliberately choose a simpler model that lends itself to a dynamic programming approach.

\citet{fang2015embodied} present a model to collaboratively generate a referring expression, incorporating feedback from the human subject to generate additional terms. The paper focuses on the generation of referring expressions and the use of gestural feedback, and so is of limited use in the context of this paper.

In addition to these, we rely on previous work about prepositional phrases:

\citet{collins95} present a model for disambiguating prepositional phrase attachments. They deal with Noun-Phrases and Verb-Phrases, but their statistical technique may be useful. This concept is also explored by \citet{ratna98}, and \citet{brill94}, each of whom suggest alternative models. Additionally, \citet{merlo97} explore disambiguating multiple prepositional phrases, rather than a single phrase between multiple targets, both of which are relevant to future work. However, our current model uses sentences focused around a single noun phrase, unlike the general English corpus used in this paper. Additionally, the incremental nature of our parsing requires an alternative framework, and as such not all techniques suggested in these papers can be used.

Another issue we face is the challenge of identifying objects and mapping them to probability distributions via language model. To deal with this, \citet{barbu13} present a model for mapping language to object models in video data and \citet{UW_RSE_ICML2012} describe an approach to the problem of simultaneously observing and extracting representations of a perceived world. These approaches can be adapted to help design features, choose objects, and select language models for prepositional phrase training, rather than using pre-defined objects and locations as we currently do, similar to the work of \citet{tellex2011understanding} which presents a method to dynamically generate a probabilistic model of a natural language input and perform inferences relating to semantic meaning. This is similar to the work we will preform in semantic parsing, though we will do so incrementally rather than with an entire sentence and thus our work will need to modify these models.

There are still a number of improvements to be made to our machine learning techniques. \citet{rudzicz03} give us a framework for parsing and understanding prepositional phrases. Similarly, \citet{liang2013learning} describe a method to create a semantic parser using question-answer pairs as data, rather than requiring annotated sentences, solving the same issue we approach of transforming natural language into semantic meaning. However, while these may be useful for generating a semantic model over entire sentences our technique will be different as we are currently only working with noun phrases, and apply additional information from the spatial model. 

\section{Conclusion}
In this paper we have presented an incremental referring expression parser that can process prepositional phrases. The incremental nature of the parser is the key contribution: state-of-the-art parsers all operate on complete referring expressions. % The incremental nature of this parser allows for lower-latency interpretations of referring expressions, and opens the possibility of integration with social feedback models in the future.

The primary future task is to integrate this with the social feedback framework on Baxter in the H2R lab and conduct user studies to investigate if this provides a measurable improvement to user interaction. Other future work includes developing a model of the preposition `near' that better matches human sensibilities. This may even extend to learning the spatial shift between humans and our system and adjusting spatial models to account for that.

\clearpage
\printbibliography

\end{document}

