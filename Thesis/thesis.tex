\documentclass[conference]{IEEEtran}
%\documentclass[letterpaper,10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{kpfonts}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{parskip}
\usepackage{tabularx}
\usepackage{qtree}
\usepackage{graphicx}
\usepackage{xfrac}
\usepackage{forest}
\usepackage{tikz-dependency}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{xfrac}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage{pbox}
\usepackage{framed}
\usepackage[authordate,bibencoding=auto,strict,backend=biber,natbib]{biblatex-chicago}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{float}
\usepackage{dblfloatfix}
\usepackage{afterpage}
\usepackage{multirow}
%\usepackage[section]{placeins}


\renewcommand{\cite}{\citep}
\numberwithin{equation}{section}
\renewcommand{\tabularxcolumn}[1]{>{\small}m{#1}}

\def\topfraction{0.9} % 90 percent of the page may be used by floats on top
\def\bottomfraction{0.9} % the same at the bottom
\def\textfraction{0.01}


%\pdfinfo{
%   /Author (Gaurav Manek)
%   /Title  (Incrementally Identifying Objects from Referring Expressions using Spatial Object Models)
%   /CreationDate (D:20160210120000)
%   /Subject (Incremental Parsing)
%   /Keywords (Incremental;Parsing;Referring Expressions;Prepositions)
%}

%\newcommand{\sectionbreak}{\clearpage}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% http://tex.stackexchange.com/a/33547
\newcommand{\approptoinn}[2]{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    #1\propto\cr\noalign{\kern2pt}#1\sim\cr\noalign{\kern-2pt}}}}}

\newcommand{\appropto}{\mathpalette\approptoinn\relax}

\addbibresource{prop.bib}

%opening
\title{Incrementally Identifying Objects from Referring Expressions using Spatial Object Models}
\author{Gaurav Manek\\(Advisor: Prof. Stefanie Tellex)}

\begin{document}

\maketitle

\section{Abstract}
An important problem in human-robot interaction is that of referring expressions: phrases used to identify a particular object among others. Existing parsing models all operate on entire sentences. Incrementally parsing referring expressions is important for human-robot interaction and conversational feedback. We present a model for parsing real-word referring expressions, trained and tested on human-provided data. In our test corpus, when presented with the entire sentence, our model ranks the correct object as the most likely 60.3\% of the time, and ranks the correct object in the top three 79.0\% of the time. Given the entire sentence humans identify the correct object 79.0\% of the time. With 50\%, 80\% and 90\% of the sentence, the model ranks the correct object as the most likely 17.4\%, 27.8\%, and 36.2\% of the time.


\section{Introduction}

Referring expressions are phrases used to identify a particular object in a scene by describing it and its relative position to other objects. The integration of \emph{social feedback}, where the robot shows its understanding of a human's utterances by generating small responses as it listens to the human, can prompt clarifications from the human and improve the accuracy of referring expression parsing in interactive contexts. However, this requires robots to be able to parse the human input \emph{incrementally}: updating its understanding as each next word is uttered. 


\begin{figure}[!tb]
  \centering
    \fbox{\includegraphics[width=0.9\columnwidth]{baxter}}
  \label{fig:robot}

  \caption{Baxter Robot in H2R lab, Brown University.}
\end{figure}

In current work, referring expression parsing is done in batch-mode, with the entire referring expression as input. \citep{tellex2011understanding,UW_RSE_ICML2012,artzi2013weakly,fang2015embodied} During interactive use, batch-mode requires waiting for the complete utterance before processing and providing output, which introduces unaccceptable latency in the robot's response. For example, practical implementations of the $G^3$ system, as created by \citet{tellex2011understanding}, can take up to 30 seconds from the end of the input to the start of a response. Our incremental parsing system updates the distribution with each added word, substantially reducing the delay between input and response.

In this paper we present an incremental referring expression parser that can process prepositional phrases. The incremental parser works by using a conditional-random field chunker to add parts of speech tags to sentences. These tags are used to construct a parse tree, which is then evaluated using an object-word model to resolve references to objects and a preposition model to resolve prepositional phrases. A caching method avoids recomputation cost and gives good worst-case time guarantees.

\afterpage{%
\begin{figure*}[tb]
\newcommand{\inctest}[1]{\includegraphics[trim={0 3cm 5cm 3.5cm},clip,width=0.5\textwidth]{test_set_example/Slide##1}}
\centering

\newcolumntype{L}{>{\centering\arraybackslash}m{0.5\textwidth}}
\begin{tabular}{LL}
\inctest{5} & \inctest{6} \\
   ~
& \emph{``--''} \\
  Cube 4 is the target of the referring expression. 
& Without any information, distribution is uniform over all objects. \\[1em]\hline\\[1em]
\inctest{7} & \inctest{9} \\
  \emph{``Orange--''}
& \emph{``Orange cube between--''} \\
  The first word increases the probability of all cubes and reduces the probability of all non-cubes.
& Symmetry of the distribution is broken by the preposition because some cubes are between more pairs of objects than other cubes. This occurs because we marginalize over all possible pairs of objects that between can be grounded on. \\[1em]\hline\\[1em]
\inctest{11} & \inctest{15} \\
  \emph{``Orange cube between the toy--''}
& \emph{``Orange cube between the toy and the tape roll.''} \\
  The direct provision of one of between's groundings shifts the distribution towards cubes that are between  the toy plane and other cubes.
& At the end of the input, cube 4 has the highest assigned probability.
\end{tabular}

\vspace{0.5in}
\caption{Example incremental evaluation -- Example \#368 from the test set, proceeds in reading order.}
\label{fig:worked_example}
\end{figure*}

\clearpage
}

We evaluate our model on novel real-world data and show that it assigns the correct object the highest probability 32.8\% of the time and in the top-3 objects 63.5\% of the time. In comparison, humans correctly identify the object 79.0\% of the time, a unigram model 16.1\% of the time and random selection is only correct 7.2\% of the time. We also show how incremental performance converges to the above values as more words are available. Figure~\ref{fig:worked_example} shows an example of the incremental parsing of a referring expression. As more words are available to the parser, the parser identifies the target object by assigning it the highest probability.


\section{Related Work}

Prepositional phrases have not been subject to as much computational analysis and study as noun- and verb-phrases. The current research on the topic uses referring expressions as parts of larger phrases, such as commands. There is an existing family of related work by \citet{tellex2011understanding}, \citet{UW_RSE_ICML2012}, and \citet{artzi2013weakly}, all of whom present modern models to process referring expressions. These models all operate on entire input sentences and are designed to parse general instructions and commands instead of only prepositional phrases. 
\citet{tellex2011understanding} present the $G^3$ framework. We use several key ideas from this paper: in particular we implicitly assume the binary correspondence variable that their model maximizes. Our algorithm is inspired in part by the algorithm they present.
\citet{UW_RSE_ICML2012} present a state-of-the-art process to learn models for a semantic parser and word-classifier alignment. Our approach is substantially different from \citeauthor{UW_RSE_ICML2012}1. since we do not separate perceptual features from the language model of each object. Also, in the learning phase of their algorithm, they calculate the marginal probability of a particular grounding and a particular word by performing a beam search over all possible parses. We assume instead that each node in the parse is independent of its sibling nodes, which allows us to use dynamic programming to incrementally build the distribution.
\citet{artzi2013weakly} train Combinatory Categorial Grammars (CCG) with ambiguous validation functions to parse instructions, including spatial relations. While this approach is more flexible and likely performs better on entirely novel sentences, we deliberately choose a simpler model that lends itself to a dynamic programming approach.

\citet{fang2015embodied} present a model to collaboratively generate a referring expression, incorporating feedback from the human subject to generate additional terms. The paper focuses on the generation of referring expressions and the use of gestural feedback, and so is of limited use in the context of this paper.

In addition to these, we rely on previous work about prepositional phrases: \citet{collins95} present a model for disambiguating prepositional phrase attachments. They deal with Noun-Phrases and Verb-Phrases, but their statistical technique may be useful. This concept is also explored by \citet{ratna98}, and \citet{brill94}, each of whom suggest alternative models. Additionally, \citet{merlo97} explore disambiguating multiple prepositional phrases, rather than a single phrase between multiple targets, both of which are relevant to future work. However, our current model uses sentences focused around a single noun phrase, unlike the general English corpus used in this paper. Additionally, the incremental nature of our parsing requires an alternative framework, and as such not all techniques suggested in these papers can be used.

Another issue we face is the challenge of identifying objects and mapping them to probability distributions via language model. To deal with this, \citet{barbu13} present a model for mapping language to object models in video data and \citet{UW_RSE_ICML2012} describe an approach to the problem of simultaneously observing and extracting representations of a perceived world. These approaches can be adapted to help design features, choose objects, and select language models for prepositional phrase training, rather than using pre-defined objects and locations as we currently do, similar to the work of \citet{tellex2011understanding} which presents a method to dynamically generate a probabilistic model of a natural language input and perform inferences relating to semantic meaning. This is similar to the work we will preform in semantic parsing, though we will do so incrementally rather than with an entire sentence and thus our work will need to modify these models.

There are still a number of improvements to be made to our machine learning techniques. \citet{rudzicz03} give us a framework for parsing and understanding prepositional phrases. Similarly, \citet{liang2013learning} describe a method to create a semantic parser using question-answer pairs as data, rather than requiring annotated sentences, solving the same issue we approach of transforming natural language into semantic meaning. However, while these may be useful for generating a semantic model over entire sentences our technique will be different as we are currently only working with noun phrases, and apply additional information from the spatial model. 


\section{Technical Approach}
\newcommand{\true}{\top}
\newcommand{\false}{\bot}
Given a sequence of words $\lambda_1, \lambda_2, \ldots, \lambda_t$, we estimate the distribution over the objects, of the objects that the user is referring to as $\Gamma$, which is a distribution over $\xi$ (the set of all objects on the table). $\gamma$ refers to an arbitrary object in that distribution. We evaluate:
\begin{align}
	& \argmax_{\gamma \in \xi} \Pr(\Gamma = \gamma | \lambda_1, \lambda_2, \ldots, \lambda_t) \
\end{align}

We reformulate the problem with correspondence variable $\Phi$, which is true if and only if the object $\gamma$ is correctly identified by $(\lambda_1, \lambda_2, \ldots, \lambda_t)$.  \citep{tellex2011understanding}
\begin{align}
	& \argmax_{\gamma \in \xi} \Pr(\Phi = \true | \Gamma = \gamma, (\lambda_1, \lambda_2, \ldots, \lambda_t))
\end{align}

We then assume that we can factor the sequence of words into separate independent constituents ($\Lambda_1, \Lambda_2, \ldots, \Lambda_k$), according to the compositional structure of language \citep{heim1998semantics}, each of which corresponds to either a grounding (a description of the object, such as ``the orange cube'') or a prepositional phrase (e.g. ``between the \ldots''). We assu

We assume these to be independent and factor the expression:
\begin{align}
   &\Pr(\Phi = \true | \Gamma = \gamma, \lambda_1, \lambda_2, \ldots, \lambda_t)
\\ & = \Pr(\Phi = \true | \Gamma = \gamma, \Lambda_1, \Lambda_2, \ldots, \Lambda_k) 
\intertext{Applying Bayes' rule:}
 & = \frac{\Pr(\Gamma = \gamma, \Lambda_1, \Lambda_2, \ldots, \Lambda_k | \Phi = \true)  \cdot \Pr(\Phi = \true)}{\Pr(\Gamma = \gamma, \Lambda_1, \Lambda_2, \ldots, \Lambda_k)}
\intertext{We assume a uniform prior on the correspondence variable, and so eliminate it. Since the algorithm performs this calculation for all possible groundings $\gamma$ and normalizes the distribution, we can rewrite the expression as:}
 & \propto \Pr(\Gamma = \gamma, \Lambda_1, \Lambda_2, \ldots, \Lambda_k | \Phi = \true)\label{eqn:assumeuniformprior}
\intertext{We have assumed that the constitutents are independent, so we separate the terms. Each constituent has its own correspondence variable $\phi_i$ that is true if and only if that constituent correctly identifies the target object $\gamma$.}
 & = \prod_{i=1}^k \Pr(\Gamma = \gamma, \Lambda_i | \phi_i) \label{eqn:combinedistribs}
\end{align}

This factorization is done using a chunking algorithm, \citep{McCallumMALLET}. We assume that the chunkings given here are certain, and so eliminate the probability term associated with that. For speed, we approximate with a chunking which has been shown to give good results in practice.

\subsection{Incremental Parsing Algorithm}

The algorithm we present uses the factorization presented above but operates in a bottom-up manner. We represent the referring expression as a tree, updating it each time we receive the next word from the user. This representation allows us to perform computationally-intensive tasks only once and cache intermediate results, allowing us to produce intermediate results without having to recompute them.

More precisely, we construct a semantic tree such that each leaf node in the tree is a noun-phrase that refers to some object. This tree is constructed by chunking the input using conditional random fields and then using deterministic transformations to turn the chunked input into a tree. We convert each of the leaf nodes into distributions over objects using a language model, and then finally evaluate this structure to obtain the final distribution.

\begin{figure*}[b]
  \centering
  \small
  \setlength{\qtreepadding}{1pt}
\begin{tabular}{ccccccc}\hline\hline
\Tree [.{$\Pr(\Lambda, \Gamma | \Phi)$} \emph{The orange cube} [.{\emph{between}} \emph{the red} \emph{the yellow} ]] \vspace{0.12in} &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.{$\Pr(\Lambda, \Gamma | \Phi)$} {$\Pr(\Lambda_1, \Gamma | \phi_1)$} [.{Preposition $p \in P$} {$\Pr(\Lambda_{2,1}, \Gamma | \phi_{2,1})$} {$\Pr(\Lambda_{2,2}, \Gamma | \phi_{2,2})$} ]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.{$\Pr(\Lambda, \Gamma | \Phi)$} {$\Pr(\Lambda_{1}, \Gamma|\phi_1)$} {$\Pr(\Lambda_{2}, \Gamma|\phi_2)$} ]
&
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} & 
\Tree [.{$\Pr(\Lambda, \Gamma | \Phi)$} ]
\\ \hline
\multicolumn{2}{r}{\textbf{(Case 0)}} & \multicolumn{2}{r}{\textbf{(Case 1)}} & \multicolumn{2}{r}{\textbf{(Case 2)}}
\end{tabular}
\caption{The three cases of bottom-up evaluation.}
  \label{fig:bottom_up_eval}
\end{figure*}


\subsection{Chunking and Semantic Tree construction}
\label{sec:tagging}

The algorithm's input is a sequence of words which needs to be transformed to a semantic tree for use with later stages. The first stage in this transformation is to assign a tag to each word that is similar to parts-of-speech tags. Instead of using the full set of English parts of speech, we use a reduced set developed for this  application.

We model the relationship between words and tags as a conditional random field, where the tag for any particular word depends on the neighboring words. We directly estimate the distribution of tags using the existing library Mallet, developed by \citet{McCallumMALLET}. The transformation from the tagged sequence to the semantic tree is entirely deterministic, as the tags are tailored to the specific form of the queries in the corpus.

\subsection{Bottom-up Evaluation}
\label{sec:bottomupeval}
Once we have a semantic tree, we can simplify the tree in a bottom-up manner to obtain the final distribution. Figure~\ref{fig:bottom_up_eval} illustrates this process and the three possible cases we apply to convert the tree into a distribution over objects.

\textbf{\textsc{Case 1}} Estimating the distribution of a grounding.

Each grounding in the tree is modeled by a distribution that is obtained from the language model. 
\begin{align}
   \Pr(\Gamma = \gamma, \Lambda_i | \phi_i = \true)
& = \prod_{\lambda \in \Lambda_j} \left( \frac{Q(\gamma, \lambda) + \alpha}{\sum_{\omega} Q(\omega, \lambda) + \alpha \cdot |\xi|} \right) \label{eqn:unigram_model}
\end{align}

The language model used is a unigram language model, where each word $\lambda$ refers to object $\gamma$ with some joint score $Q(\gamma, \lambda)$. In our model, we set $Q(\gamma, \lambda)$ to be the number of times that $\lambda$ was used to describe $\gamma$ in our training set. The distribution of simple noun phrase $\Lambda_j$ referring to object $\gamma$ is given by Equation~\ref{eqn:unigram_model}. We use add-alpha smoothing arbitrarily setting $\alpha \approx 5\%$. $\xi$ is the set of all objects, and $|\xi|$ is the number of objects.

Our implementation will be able to perform a lookup in $\mathcal{O}(|\Lambda_j|)$ time, where $|\Lambda_j|$ is the number of words in $\Lambda_j$.

\textbf{\textsc{Case 2}} Simplifying a preposition and associated noun-phrases.

We have preposition $p \in P = $ \{`near', `left', `right', `front', `behind', `between'\}. We discuss the selection of these in Section~\ref{sec:prepselect}.

\begin{figure*}[!b]
  \small
  \centering
\begin{tabular}{ccccc}\hline\hline
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache miss} \emph{The orange cube} ] ]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache hit} \emph{The orange cube} ] [.\fbox{cache miss} [.{\emph{between}} [.\fbox{cache miss} \emph{the red} ]]]]] &
\pbox{0.2in}{\vspace{0.5in}
$\Rightarrow$} &
\Tree [.$\circ$ [.\fbox{cache miss} [.\fbox{cache hit} \emph{The orange cube} ] [.\fbox{cache miss} [.{\emph{between}} [.\fbox{cache hit} \emph{the red} ] [.\fbox{cache miss} \emph{the yellow} ]]]]]
\\ \hline
``The orange cube & & \ldots between the red  & & \ldots and the yellow.''
\end{tabular}
\caption{Cache behavior as words are added.}
  \label{fig:incremental_caching}
\end{figure*}

We begin with $\Pr(\Gamma = \gamma, \Lambda_i | \phi_i = \true)$. We use the structure of prepositional phrases to factor $\Lambda_j$ into a preposition and groundings. $\Lambda_{j,P}$ refers to the subset of $\Lambda_{j}$ which describes the preposition (e.g. the word ``between'' or ``near''), and corresponds to $P$. Each $\Lambda_{j,i}$ refers to the subset of $\Lambda_{j}$ which describes the $i^\text{th}$ object that the preposition references (i.e. the object to which the target is ``near'') and corresponds to the distribution over all objects $\Gamma_{j,i}$. Let $n$ be the number of such groundings.
\begin{align}
 & = \Pr(\Gamma = \gamma, \Lambda_{j,P},\Lambda_{j,1}, \ldots, \Lambda_{j,n}| \phi_i = \true)
\intertext{We apply the law of total probability and introduce terms for each possible grounding.}
\begin{split}
& \sum_{\gamma_1 \in \xi} \cdots\sum_{\gamma_n \in \xi}
\\ & \phantom{=}~\Pr(\Gamma = \gamma,  \Lambda_{j,P}, \Gamma_{j,1} = \gamma_i, \ldots, \Gamma_{j,n} = \gamma_n| \phi_i = \true)
\\ & \phantom{=}~\cdot \Pr(\Lambda_{j,1}, \ldots, \Lambda_{j,n} | \Gamma_{j,1} = \gamma_1, \ldots, \Gamma_{j,n} = \gamma_n)
\end{split}
\intertext{We once again invoke the concept of correspondence variables, letting $\phi_{j,i} = \true$ if $\Gamma_{j,i} = \gamma_i$ corresponds to $\Lambda_{j,i}$. We further assume that each grounding is independent, which allows us to separate the grounding term:}
\begin{split}
& = \sum_{\gamma_1 \in \xi} \cdots\sum_{\gamma_n \in \xi} \left( \prod_{i = 1}^n\Pr(\Lambda_{j,i}, \Gamma_{j,i} = \gamma_i | \phi_{j,i} = \true) \right)
\\ & \phantom{=}~\Pr(\Gamma = \gamma,  \Lambda_{j,P} \Gamma_{j,1} = \gamma_i, \ldots, \Gamma_{j,n} = \gamma_n| \phi_i = \true)
\end{split}
\intertext{The last term relates all the groundings and the target object with the preposition used. We parametrize it with feature-vector function $f$, weights $\theta_p$, logistic function $S$, and some normalization factor $z$. $\theta_p$ is calculated by performing logistic regression, with the feature-vector functions $f$ detailed in \ref{sec:featurevectors}.}
\begin{split}
& = \sum_{\gamma_1 \in \xi} \cdots\sum_{\gamma_n \in \xi} \left( \prod_{i = 1}^n\Pr(\Lambda_{j,i}, \Gamma_{j,i} = \gamma_i | \phi_{j,i} = \true) \right)
\\ & \phantom{=}~\text{S}\left( \frac{f(\gamma, \gamma_1, \ldots) \cdot \theta_p}{z} \right)
\end{split}
\end{align}

We observe that our na\"ive implementation takes time to the order of $\mathcal{O}(|\xi|^{n+1})$, where $|\xi|$ is the number of objects and $n$ is the number of groundings that each preposition has.


\textbf{\textsc{Case 3}} Combining multiple distributions.

In Figure~\ref{fig:bottom_up_eval}, we simplify groundings $Pr(\Lambda_1, \Gamma = \gamma | \phi_1)$ and derived distribution $Pr(\Lambda_2, \Gamma = \gamma | | \phi_2)$ to get $\Gamma$, our estimated distribution. As established in the initial factorization, we simply take the inner product of all distributions to find the overall distribution. Equation~\ref{eqn:combinedistribs} is reproduced here:
\begin{align*}
& \Pr(\Gamma = \gamma | \lambda_1, \lambda_2, \ldots, \lambda_t) = 
\prod_{i=1}^k \Pr(\Gamma = \gamma, \Lambda_i | \phi_i)\end{align*}

The na\"ive implementation also takes time to the order of $\mathcal{O}(|\xi|^{n+1})$, where $|\xi|$ is the number of objects and $n$ is the number of groundings for which the marginal distribution must be taken.

\subsection{Preposition Choice}
\label{sec:prepselect}

To select the final set of prepositions $P = $ \{`near', `left', `right', `front', `behind', `between'\}, we performed an online test with humans to identify the prepositions people naturally use when performing such tasks. We gathered 68 prepositions over 58 sentences, and grouped them by meaning. Figure~\ref{fig:prepositionlist} contains the exhaustive list.

\begin{figure}[h!]
  \small
  \centering
  \raggedright
  \setlength\tabcolsep{2pt}
  \begin{tabular}{|r c lllll|}\hline
  33 & \textbf{between} & between &&&& \\
  18 & \textbf{near} & closest (to) & next (to) & near & nearer & closer  \\
  7 & \textbf{in front} & in front (of) & below & bottom && \\
  4 & \textbf{left} & left &&&& \\
  3 & \textbf{behind} & top (of) & behind &&& \\
  3 & \textbf{far} & far (from) & far away & farthest && \\\hline
  \end{tabular}
  \caption{The prepositions people naturally use, characterized by an initial test. The prepositions are grouped by meaning, with the frequency of each group on the left. The exact usage is shown decreasing in frequency from left to right.}
  \label{fig:prepositionlist}
\end{figure}

From this test, the initial set of prepositions was selected to be \{`near', `far', `left', `right', `front', `behind', `between'\}, including the preposition ``right'' as the counterpart to ``left''. After gathering more data, the lack of data forced us to drop `far'. 

We included the list of prepositions in the prompt to encourage people to use them in their input, which likely limited the number of other prepositions in the training set: manual inspection of the training set revealed no significant unrecognized prepositions. See Section~\ref{sec:datacollection} for details on the collection and human evaluation of referring expressions.

\subsection{Feature-Vector Functions}
\label{sec:featurevectors}

Before features are computed, all scenes are scaled so that longer axis lies from 0 to 1. The aspect ratio of axes is maintained.

For all one-place prepositions (`near', `left', `right', `front', `behind'), we use only three features. The target object is located at $(x_\gamma, y_\gamma)$, and the object referred to by the preposition is at $(x_1, y_1)$. 
\begin{flushleft}
\begin{enumerate}
\item The difference in the x-coordinate: $f_1((x_\gamma, y_\gamma), (x_1, y_1)) = x_\gamma - x_1$
\item The difference in the y-coordinate: $f_2((x_\gamma, y_\gamma), (x_1, y_1)) = y_\gamma - y_1$
\item The Cartesian distance: $f_3((x_\gamma, y_\gamma), (x_1, y_1)) = \sqrt{(y_\gamma - y_1)^2 (x_\gamma - x_1)^2}$
\end{enumerate}
\end{flushleft}

For the only two-place preposition (`between'), we first   draw a line connecting the two grounding objects. We project the point corresponding to the target object onto that line, and compute the distance from the midpoint along the line and perpendicular to the line, scaled by the distance between the two objects. 
We have three different features, all in terms of the parallel distance $p$ and the perpendicular distance $q$: 
\begin{flushleft}
\begin{enumerate}
\item The parallel distance: $f_1(p, q) = |p|$
\item The perpendicular distance: $f_2(p, q) = |q|$
\item The smoothed product of the distances: $f_1(p, q) = (|p| +\epsilon)\cdot(|q| + \epsilon)$, with arbitrary $\epsilon = .1$.
\end{enumerate}
\end{flushleft}

\subsection{Incremental Parsing}

In the previous section we factored the simplification of the semantic tree into three separate cases. We cache the result of each simplifying step, as illustrated in Figure~\ref{fig:incremental_caching}.


\subsubsection{Runtime Analysis}

We use the runtime analysis of each separate case to draw conclusions about the worst-case time taken for the algorithm to produce an updated distribution given one additional word.

Given the addition of one word, we need to make at most one more recursive simplification than the depth of the tree. In all our training data, the maximum tree depth observed is never more than two, so an upper bound of three simplifications per word input means that this algorithm can easily meet the runtime requirements of online algorithms.

More formally, given the runtimes and caching behavior discussed earlier, the worst-case time to update the distribution $\Gamma$ to include the next word from the user is $\mathcal{O}(|\Lambda|) \prod_i^{k-1} \mathcal{O}(|\xi|^{n+1}) = \mathcal{O}(|\Lambda| * |\xi|^{kn})$, where $k$ is the number of layers in the tree.

When $k = 3$ and $n \leq 2$, as in real-world examples, this instead evaluates to the very manageable $\mathcal{O}(|\Lambda| * |\xi|^{4})$, where $|\Lambda|$ is the number of words in the simple noun phrase, and $|\xi|$ is the number of objects in the scene.



\subsubsection{Chunking with Conditional Random Fields}
\begin{figure}[h!]
  \centering   
  \label{fig:crf_retagging_hist}

\begin{tikzpicture}
\begin{axis}[
	ybar stacked,
	width=0.7\columnwidth,
	height=1in,
	scale only axis,
	xtick=data,
	xlabel={No. of tags changed},
	ylabel={Frequency},
	point meta={y*100},
	nodes near coords={\scriptsize\pgfmathprintnumber\pgfplotspointmeta\%},
	yticklabel={\pgfmathparse{\tick*100}\pgfmathprintnumber{\pgfmathresult}\%},
	try min ticks={4},
%	x label style={yshift=10pt},
	y label style={},
	ymin=0.0,ymax=1.0,
	xmin=-0.5, xmax=9.5,
    bar width=10,
    log basis y=10,
    ymajorgrids
]

\addplot [fill=orange]
	coordinates {(0, .87)	(1, .075)	(2, .031)	(3, .009)	(4, .012)	(5, .001)	(6, .001)	(7, 0)	(8, 0)	(9, 0)};

\end{axis}
\end{tikzpicture}

\caption{Distribution of number of tags changed each time the chunker is run with one additional word.}
\end{figure}


\begin{figure*}[!tb]
  \centering
  \setlength\extrarowheight{0pt}
  \fbox{
  \begin{tabularx}{0.95\textwidth}{@{}c X@{}}
  \multirow{6}{0.95\columnwidth}{\includegraphics[width=0.95\columnwidth]{ref_expr_examples_pic}}
    & Examples of responses from AMT workers: \\
	& ``I want the orange cube in front of you, between the chili and the toy.'' \\
	& ``hand me the orange cube that is in between jalape\~{n}o and airplane'' \\
	& ``Directly in front of you next to the green pepper and airplane.'' \\
	& ``Take the orange block between the toy airplane and green chili pepper'' \\
	& ``bring the orange cube between the pepper and the plane.'' \\ 
   	\end{tabularx}
    }
	\caption{Example picture provided to AMT workers to elicit referring expressions and sample responses.}
	\label{fig:ref_expr_examples_pic}
\end{figure*}

The use of conditional random fields complicates the analysis somewhat because adding a word can affect the tags of words already processed, which causes the tree structure to change. In this case, we recompute all nodes in the tree corresponding to words with changed tags, paying the recomputation penalty. Other models such as Hidden Markov Models also have this property.


We choose to use Conditional Random Fields in part because we can explicitly set the look-ahead limit, and so pick a tradeoff between tagging accuracy and potential worst-case performance. For our algorithm, we arbitrarily set it to 5.

The worst-case runtime scales linearly with the look-ahead limit. To characterize the typical performance penalty caused by our choice of look-ahead parameter, we gathered statistics on the likelihood of this happening in our training set.

We found that, in our test set, each time a word is added an average of 0.23 earlier tags are changed. The observed distribution is presented in Figure~\ref{fig:crf_retagging_hist}. The $95^\text{th}$ percentile case is still within the same order of magnitude as the case without any changes.


\section{Evaluation} 

We collected a corpus of human-generated data using Human Intelligence Tasks (HITs) on the Amazon Mechanical Turk (AMT) platform and hand-generated scenes. We additionally use AMT to have humans evaluate our test set to obtain a baseline. We trained the language model and spatial-prepositional model on a training set of 11 scenes, each with an average of 14 objects, for a total of 417 input sentences. We trained the chunker on hand-annotated parses of these input sentences.


\subsection{Data Collection}
\label{sec:datacollection}
A set of HITs were created to elicit referring expressions. A total of 19 scenes were constructed, each with a set of about 12 to 15 objects scattered on a table and at least 6 identical orange cubes. For each orange cube in each scene, 9 different workers were told to ask a robot across the table for the indicated orange cube. Figure~\ref{fig:ref_expr_examples_pic} provides an example of such a labeled scene.

We provide the following instructions:

\begin{framed}
You are standing across the table from a robot. Write down what you would say to the robot if you wanted the indicated orange cube on the table.
\begin{itemize}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex,leftmargin=1em]
\item Use phrases like "between", "near", "left of", "right of", "in front of" and "behind".
\item Use front/behind/left/right from the robot's perspective, as labeled in the image. 
\item All orange cubes look the same.
\item You must ask for the item indicated by the red arrow
\item The instruction must be a single sentence.
\end{itemize}
\end{framed}

For each referring expression in the test set, we get three separate human raters to identify the target and provide feedback on the ease of understanding of the referring expression. Refer to Figure~\ref{fig:results} for how often humans correctly identify the target, and Figure~\ref{fig:interrater} for interrater agreement.  

\begin{comment}
\begin{figure}[!h]
  \centering
    \fbox{\includegraphics[width=0.95\columnwidth]{ref_expr_eval_pic}}
  \caption{Example picture provided to AMT workers to evaluate referring expressions.}
  \label{fig:ref_expr_eval_pic}
\end{figure}
\end{comment}

\begin{figure*}[!b]
  \centering
  \begin{tabular}{| r r | c | c | c || c c |} \hline
     & & \multicolumn{5}{c|}{Rate of correct identification, Test (\%)} \\
     & & \multicolumn{3}{c||}{Baselines} & \multicolumn{2}{c|}{Results} \\
     \multicolumn{2}{|c|}{Preposition}
			                      &   Human & Unigram & Random &  Top-1 & Top-3 \\\hline
26.8\% & \textit{between}       & 88.2  & 14.3  & 7.1   & 77.5  & 97.1 \\
21.3\% & \textit{near}          & 82.5  & 17.2  & 7.3   & 64.7  & 88.1 \\
14.2\% & \textit{behind}        & 76.9  & 15.7  & 6.9   & 81.5  & 94.4 \\
11.0\% & \textit{in front of}   & 69.9  & 17.9  & 7.5   & 57.1  & 88.1 \\
9.4\% & \textit{left of}        & 89.8  & 16.1  & 7.3   & 69.4  & 100.0 \\
5.8\% & \textit{right of}       & 58.0  & 15.4  & 7.3   & 55.2  & 91.6 \\\hline\hline
    \multicolumn{2}{|r|}{Total} & 79.0  & 16.1  & 7.2   & 60.3  & 79.0 \\\hline
  \end{tabular}
  \caption{Performance on test set.}
  \label{fig:results}
\end{figure*}

\subsection{Results}

After training our model on the training set, we tested it using a test set of 10 scenes, each with an average of 14 objects, for a total of 381 input sentences. The result of running the parser on complete referring expressions is in Figure~\ref{fig:results}.

Figure~\ref{fig:results} shows the correctness rate of our algorithm and of three baselines for comparison. The percentage next to each preposition is the fraction of sentences in the test set that contain this preposition, and so will not add up to 100\%. The baselines are:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
	\item The \emph{Human} baseline, which was established by having humans select the object best identified by the referring expression, and scoring them against our corpus.
	\item The \emph{Unigram} baseline, which is the expectation of selecting the correct object using a simple unigram object model across the entire input sentence.
	\item The \emph{Random} baseline, which is the expectation of selecting the correct object by selecting one object uniformly at random.
\end{enumerate}

The \emph{Results} column lists the rate of correct identification using the entire sentence as input, the \emph{Top-1} column lists the rate at which the correct object is rated the most likely by the algorithm, and the \emph{Top-3} column lists the rate at which the correct object is in the top 3 items.

For evaluation, the rate of correct identification is the number of trials in which the algorithm assigns a higher probability to the correct item than to any other item. Should there be a tie, the rate is divided by the number of items of equal probability.

The percentage on the left of each preposition is the fraction of the test set that contains that preposition.

To evaluate incremental performance, we report performance on the test set as a fraction of each sentence provided to the algorithm. Figure~\ref{fig:results_inc} reports the evaluation of the test set when our algorithm is run on it word-by-word. Note that, due to the variation in lengths of sentences, the charts are drawn by interpolating fraction of each sentence into bins from 0 to 1. Because of the structure of the algorithm, incremental evaluation converge to the batch-mode results after the final word in the sentence is presented.

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{eval/cdf_by_fraction}
  \caption{Performance on test set.}
  \label{fig:results_inc}
\end{figure}

Five separate lines are drawn to show the distribution of the rank of the correct option. Each Top-$k$ line includes an example if the target object has at least as much probability as the $k^\text{th}$-highest probability in the distribution. Should there be a tie, the rate is divided by the number of items of equal probability.

\subsection{Imprecise and Incorrect Data}

A major source of error is the presence of imprecise or incorrect data in our training and test set. An imprecise referring expression is one that does not uniquely identify a target object, and an incorrect referring expression is one that does not identify the correct object. Here we estimate how much of our error is due to imprecise and incorrect referring expressions by evaluating our algorithm on referring expressions with $r$ humans correctly identifying the target object.

Figure~\ref{fig:interrater} shows the outcome of this when we restrict the test set to referring expressions that have been correctly identified by an increasing number of humans. The first column is the control (using all data), the second includes only referring expressions correctly solved by at least one human, then at least two humans, and finally only those by all three humans.

\begin{figure}[h!tb]
  \centering
  \begin{tabular}{|r|m{1.2cm}m{1.2cm}m{1.2cm}m{1.2cm}|}\hline
  & \multicolumn{4}{c|}{Rate of correct identification, Test (\%)} \\
  $r$ & $\geq 0$ & $\geq 1$ & $\geq 2$ & $=3$ \\\hline
  Top-1 & 60.3 & 63.2 & 65.2 & 69.6 \\
  Top-3 & 79.0 & 81.0 & 82.7 & 85.1 \\\hline
  (\%) & 100 & 93.1 & 84.5 & 59.3 \\\hline
  \end{tabular}
  \caption{Correctness rate for different number of correct raters, with size of data.}
  \label{fig:interrater}
\end{figure}

From this we see that genuine confusion accounts for between 5-10 percentage points of the total error, which is substantial.

\section{Conclusion}
In this paper we have presented an incremental referring expression parser that can process prepositional phrases. The incremental nature of the parser is the key contribution: state-of-the-art parsers all operate on complete referring expressions. 

The primary future task is to integrate this with the social feedback framework on Baxter in the H2R lab and conduct user studies to investigate if this provides a measurable improvement to user interaction. Other future work includes developing a model of the preposition `near' that better matches human sensibilities. This may even extend to learning the spatial shift between humans and our system and adjusting spatial models to account for that.

From an algorithmic development perspective, a natural extension of this algorithm is to replace the chunking and semantic tree construction with a chart parser. The chart parser will provide an incremental model of parsing that our spatial model can be directly integrated into. This may even allow the spatial model to inform the parsing, such as by increasing the probability of parses that correspond to narrower distributions. The increased power of the model would be offset by the greater computational overhead: further analysis and experimentation is required to see if this is suitable for use in interactive scenarios.

\section{Acknowledgments}
This thesis project is the culmination of work done from Fall 2014 to present. Some work was previously completed in collaboration with undergraduate student Zachary Loery.

This project was completed in the H2R laboratory in the Brown computer science department, with Prof. Stefanie Tellex as advisor.

\printbibliography

\end{document}

